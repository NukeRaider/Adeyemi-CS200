{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8499c5fb-b1e0-47c4-8215-b7acaeeda884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d13dff-106a-4652-b551-348462871bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 16\n",
    "DATASET_DIR = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37d7d9d-3b81-41ad-b173-27ca0201bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153 files belonging to 2 classes.\n",
      "Using 123 files for training.\n",
      "Found 153 files belonging to 2 classes.\n",
      "Using 30 files for validation.\n",
      "Classes: ['spiral', 'elliptical']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=['spiral', 'elliptical'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=['spiral', 'elliptical'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "# Store label names\n",
    "class_names = ['spiral', 'elliptical']\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233e5cf6-bf78-450e-a846-61e66bfe3ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tucke\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255, input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3bb1e74-9296-4c95-9218-76d653f92f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098a0bcc-3dc4-4b12-8417-0042177177f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.5784 - loss: 0.7106 - val_accuracy: 0.8000 - val_loss: 0.6505\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.8382 - loss: 0.6066 - val_accuracy: 0.8000 - val_loss: 0.4769\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.9079 - loss: 0.3371 - val_accuracy: 0.7333 - val_loss: 0.6686\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - accuracy: 0.8050 - loss: 0.4706 - val_accuracy: 0.8333 - val_loss: 0.5182\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.9413 - loss: 0.1474 - val_accuracy: 0.8000 - val_loss: 0.5627\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.9334 - loss: 0.1880 - val_accuracy: 0.7667 - val_loss: 0.7886\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.9457 - loss: 0.1642 - val_accuracy: 0.7667 - val_loss: 0.6504\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 0.9929 - loss: 0.0643 - val_accuracy: 0.8000 - val_loss: 0.7869\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.9530 - loss: 0.0973 - val_accuracy: 0.8000 - val_loss: 0.7497\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.9717 - loss: 0.0787 - val_accuracy: 0.7667 - val_loss: 0.9487\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3f1f6a-2b54-4fcd-8100-66cd504e7d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9300 - loss: 0.1437\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7819 - loss: 0.7658\n",
      "Final Training Accuracy: 95.12%\n",
      "Final Validation Accuracy: 76.67%\n"
     ]
    }
   ],
   "source": [
    "final_train_loss, final_train_acc = model.evaluate(train_ds)\n",
    "final_val_loss, final_val_acc = model.evaluate(val_ds)\n",
    "\n",
    "#The closer/higher the 2 Accuracies, the better\n",
    "print(f\"Final Training Accuracy: {final_train_acc * 100:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3414c5-9ff1-4ee6-b062-12e2e3e72653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, threshold=0.6):\n",
    "    img = load_img(image_path, target_size=IMG_SIZE)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "    predictions = model.predict(img_array)[0]\n",
    "\n",
    "    # Get prediction and confidance\n",
    "    top_index = np.argmax(predictions)\n",
    "    top_confidence = predictions[top_index]\n",
    "    predicted_class = class_names[top_index]\n",
    "\n",
    "    # Irregular if not confidant\n",
    "    if top_confidence < threshold:\n",
    "        predicted_class = 'irregular'\n",
    "\n",
    "    print(f\"Prediction: {predicted_class} (Confidence: {top_confidence:.2f})\")\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ecb3aa-f201-47e5-b86f-963042e9c01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Prediction: elliptical (Confidence: 0.93)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'elliptical'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"test/988001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42203ea-3c73-49a3-8b12-01efa9b18eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Prediction: spiral (Confidence: 0.99)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spiral'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"test/987261.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dcb0b06-25e1-4258-9ad0-24fb2be5177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Prediction: elliptical (Confidence: 0.75)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'elliptical'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"test/100150.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36c68c60-84cb-4a73-8771-fd8bae10c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: spiral (Confidence: 1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spiral'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"test/101007.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29f5ba02-9f0d-42ab-8dcf-d5fa321af39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Prediction: elliptical (Confidence: 0.87)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'elliptical'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"test/101623.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bb89067-face-4d06-b563-5223578cc168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Prediction: spiral (Confidence: 1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spiral'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"test/100765.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003edb4a-5f37-4571-8ea0-2394942b1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"galaxy_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e826becb-4524-4e47-ab8b-a80941dedcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction: spiral (Confidence: 0.99)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spiral'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"test_flask_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9622db99-a639-48c5-ada6-54a1b9d67275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99491704, 0.00508303]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = load_img(\"test_flask_image.jpg\", target_size=(128, 128))\n",
    "img = img.convert(\"RGB\")\n",
    "img_array = img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "model.predict(img_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139afedc-0076-4bed-8118-67c5db01cc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
