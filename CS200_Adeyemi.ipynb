{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8499c5fb-b1e0-47c4-8215-b7acaeeda884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d13dff-106a-4652-b551-348462871bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 16\n",
    "DATASET_DIR = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37d7d9d-3b81-41ad-b173-27ca0201bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153 files belonging to 2 classes.\n",
      "Using 123 files for training.\n",
      "Found 153 files belonging to 2 classes.\n",
      "Using 30 files for validation.\n",
      "Classes: ['spiral', 'elliptical']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=['spiral', 'elliptical'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=['spiral', 'elliptical'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "# Store label names\n",
    "class_names = ['spiral', 'elliptical']\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233e5cf6-bf78-450e-a846-61e66bfe3ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tucke\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255, input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3bb1e74-9296-4c95-9218-76d653f92f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098a0bcc-3dc4-4b12-8417-0042177177f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 130ms/step - accuracy: 0.5285 - loss: 0.6912 - val_accuracy: 0.8000 - val_loss: 0.5500\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - accuracy: 0.8594 - loss: 0.4343 - val_accuracy: 0.7667 - val_loss: 0.5615\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.9101 - loss: 0.2072 - val_accuracy: 0.7333 - val_loss: 0.7017\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy: 0.8541 - loss: 0.4297 - val_accuracy: 0.7333 - val_loss: 0.6727\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 0.9513 - loss: 0.1194 - val_accuracy: 0.8667 - val_loss: 0.5676\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - accuracy: 0.9389 - loss: 0.1226 - val_accuracy: 0.8333 - val_loss: 0.6510\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 0.9784 - loss: 0.0753 - val_accuracy: 0.8333 - val_loss: 0.7017\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.9871 - loss: 0.0408 - val_accuracy: 0.8333 - val_loss: 0.8239\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - accuracy: 0.9784 - loss: 0.0579 - val_accuracy: 0.8000 - val_loss: 0.8418\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - accuracy: 0.9907 - loss: 0.0349 - val_accuracy: 0.7667 - val_loss: 1.1022\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3f1f6a-2b54-4fcd-8100-66cd504e7d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0742\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7819 - loss: 0.8551\n",
      "Final Training Accuracy: 100.00%\n",
      "Final Validation Accuracy: 76.67%\n"
     ]
    }
   ],
   "source": [
    "final_train_loss, final_train_acc = model.evaluate(train_ds)\n",
    "final_val_loss, final_val_acc = model.evaluate(val_ds)\n",
    "\n",
    "#The closer/higher the 2 Accuracies, the better\n",
    "print(f\"Final Training Accuracy: {final_train_acc * 100:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3414c5-9ff1-4ee6-b062-12e2e3e72653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, threshold=0.6):\n",
    "    img = load_img(image_path, target_size=IMG_SIZE)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "    predictions = model.predict(img_array)[0]\n",
    "\n",
    "    # Get prediction and confidance\n",
    "    top_index = np.argmax(predictions)\n",
    "    top_confidence = predictions[top_index]\n",
    "    predicted_class = class_names[top_index]\n",
    "\n",
    "    # Irregular if not confidant\n",
    "    if top_confidence < threshold:\n",
    "        predicted_class = 'irregular'\n",
    "\n",
    "    print(f\"Prediction: {predicted_class} (Confidence: {top_confidence:.2f})\")\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ecb3aa-f201-47e5-b86f-963042e9c01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "Prediction: elliptical (Confidence: 0.98)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'elliptical'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"988001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42203ea-3c73-49a3-8b12-01efa9b18eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Prediction: spiral (Confidence: 1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spiral'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"987261.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dcb0b06-25e1-4258-9ad0-24fb2be5177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: elliptical (Confidence: 0.75)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'elliptical'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"100150.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36c68c60-84cb-4a73-8771-fd8bae10c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Prediction: spiral (Confidence: 1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spiral'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"101007.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29f5ba02-9f0d-42ab-8dcf-d5fa321af39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Prediction: elliptical (Confidence: 0.95)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'elliptical'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"101623.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb89067-face-4d06-b563-5223578cc168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
