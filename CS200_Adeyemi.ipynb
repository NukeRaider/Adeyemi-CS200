{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8499c5fb-b1e0-47c4-8215-b7acaeeda884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d13dff-106a-4652-b551-348462871bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 16\n",
    "DATASET_DIR = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37d7d9d-3b81-41ad-b173-27ca0201bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files belonging to 2 classes.\n",
      "Using 80 files for training.\n",
      "Found 100 files belonging to 2 classes.\n",
      "Using 20 files for validation.\n",
      "Classes: ['spiral', 'elliptical']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=['spiral', 'elliptical'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=['spiral', 'elliptical'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "class_names = ['spiral', 'elliptical']\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "996e1b43-d83b-4f10-a943-ee855abd4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize dataset\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(100).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "233e5cf6-bf78-450e-a846-61e66bfe3ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tucke\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255, input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3bb1e74-9296-4c95-9218-76d653f92f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "098a0bcc-3dc4-4b12-8417-0042177177f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 207ms/step - accuracy: 0.5335 - loss: 0.7200 - val_accuracy: 0.4000 - val_loss: 0.7112\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - accuracy: 0.6148 - loss: 0.6335 - val_accuracy: 0.6000 - val_loss: 0.6483\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 136ms/step - accuracy: 0.7762 - loss: 0.5306 - val_accuracy: 0.7500 - val_loss: 0.5068\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 0.8849 - loss: 0.2874 - val_accuracy: 0.8500 - val_loss: 0.5963\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - accuracy: 0.9403 - loss: 0.1533 - val_accuracy: 0.7500 - val_loss: 0.7799\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 0.9469 - loss: 0.1098 - val_accuracy: 0.7000 - val_loss: 0.9474\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - accuracy: 0.9856 - loss: 0.0463 - val_accuracy: 0.6000 - val_loss: 1.3386\n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step - accuracy: 0.9845 - loss: 0.0494 - val_accuracy: 0.6500 - val_loss: 1.2935\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 146ms/step - accuracy: 0.9674 - loss: 0.0473 - val_accuracy: 0.6500 - val_loss: 1.4150\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - accuracy: 0.9741 - loss: 0.0516 - val_accuracy: 0.6000 - val_loss: 1.8010\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f3f1f6a-2b54-4fcd-8100-66cd504e7d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0178\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5667 - loss: 1.9507\n",
      "Final Training Accuracy: 100.00%\n",
      "Final Validation Accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "final_train_loss, final_train_acc = model.evaluate(train_ds)\n",
    "final_val_loss, final_val_acc = model.evaluate(val_ds)\n",
    "\n",
    "print(f\"Final Training Accuracy: {final_train_acc * 100:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed3414c5-9ff1-4ee6-b062-12e2e3e72653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, threshold=0.6):\n",
    "    img = load_img(image_path, target_size=IMG_SIZE)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "    predictions = model.predict(img_array)[0]\n",
    "\n",
    "    # Get top prediction and confidence\n",
    "    top_index = np.argmax(predictions)\n",
    "    top_confidence = predictions[top_index]\n",
    "    predicted_class = class_names[top_index]\n",
    "\n",
    "    # Fallback to irregular if not confident\n",
    "    if top_confidence < threshold:\n",
    "        predicted_class = 'irregular'\n",
    "\n",
    "    print(f\"Prediction: {predicted_class} (Confidence: {top_confidence:.2f})\")\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8ecb3aa-f201-47e5-b86f-963042e9c01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Prediction: elliptical (Confidence: 1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'elliptical'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"988001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d42203ea-3c73-49a3-8b12-01efa9b18eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: spiral (Confidence: 0.85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spiral'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"987261.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64694708-2dc0-4d2c-83fa-249d4e4be3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Prediction: spiral (Confidence: 1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spiral'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"images (2).jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "413b4123-4ad0-45ba-8a95-5e36aea1507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prediction: elliptical (Confidence: 0.90)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'elliptical'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"image0.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ece40b8-3b0a-4bdc-94b2-0ca4f847b911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Prediction: spiral (Confidence: 1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spiral'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"M104_ngc4594_sombrero_galaxy_hi-res.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed4a941c-88d2-4141-9f16-860e442d76a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Prediction: spiral (Confidence: 1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spiral'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"Kirby_Nintendo.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45e1a7-116f-43d0-8592-00b95c68d364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
